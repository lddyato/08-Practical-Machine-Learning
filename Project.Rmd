---
title: "Project_Practical Machine Learning"
author: "Yato"
date: "April 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```
# Introduction
The goal of the project is to predict the manner in which they did the exercise.  I will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. This is the "classe" variable in the training set. I will use any of the other variables to predict with the "classe" variable. The following shows how the model built by using cross validation. 

# Load the packages
```{r}
library(caret)
library(randomForest)
library(rpart)
library(rattle)
```

# Load the data
```{r}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
dim(training)
dim(testing)
```
# Data Partition
```{r}
set.seed(17717)
inTrain <- createDataPartition(y = training$classe, p = 0.6, list = FALSE)
train1 <- training[inTrain, ]
train2 <- training[-inTrain, ]
dim(train1)
dim(train2)
```
# Remove the near zero variables
```{r}
nzv <- nearZeroVar(train1)
train1 <- train1[, -nzv]
train2 <- train2[, -nzv]
dim(train1)
dim(train2)
```
# Remove the missing values
```{r}
train1 <- train1[, colSums(is.na(train1)) == 0]
train2 <- train2[, colSums(is.na(train2)) == 0]
sum(is.na(train1))
sum(is.na(train2))
```

# Remove the useless variables
```{r}
train1 <- subset(train1, select = -c(1:7))
train2 <- subset(train2, select = -c(1:7))
dim(train1)
dim(train2)
```

# Model training
Firstly set the seed so the study can be reproducible. Secondly, I consider using cross validation to predict the outcome. In general, k=5 or k= 10 is most commonly used value when doing k-fold cross validation, Here i set k=5 to save a little running time. Then I use the rpart, gbm and the random-forest technique to generate a predictive model, respectively.
```{r, fig.height=4}
set.seed(33833)
myControl <- trainControl(method = "cv", number = 5)
mod2 <- train(classe ~ ., method = "rpart", trControl=myControl, data = train1)
mod3 <- train(classe ~ ., method = "gbm", verbose = FALSE, trControl=myControl, data = train1)
mod1 <- train(classe ~ ., method = "rf", trControl=myControl, data = train1)

pred1 <- predict(mod1, train2)
pred2 <- predict(mod2, train2)
pred3 <- predict(mod3, train2)

cm1 <- confusionMatrix(train2$classe, pred1)
cm2 <- confusionMatrix(train2$classe, pred2)
cm3 <- confusionMatrix(train2$classe, pred3)
```
## The accuracy and Out-of-sample Error is obtained by the following
```{r}
Accuracy <- cbind( cm1$overall[1], cm2$overall[1], cm3$overall[1]); Accuracy
error <- 1 - Accuracy; row.names(error) <- "Out-of-Sample Error"; error
```
The accuracy of the three prediction models are 0.9943, 0.4871 and 0.9644. The out of sample errors are 0.0057, 0.5129 and 0.03556, respectively. 
From the accuracy of the three prediction models, the first model with random forest approach has the highest accuracy (0.9943) and the lowest out-of-sample error(0.0057). So i will use the first model to get the predicted result on testing data.
```{r}
mod1
cm1
plot(cm1$table, col = cm1$byClass, main = "Confusion Matrix of Random Forest model (accuracy=0.9943)")
```

# Get the final predicted result
```{r}
finalpred <- predict(mod1, testing)
finalpred
```

